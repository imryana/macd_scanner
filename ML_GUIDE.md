# ü§ñ Machine Learning Integration Guide

## Overview

Your MACD trading scanner now includes a **hybrid machine learning system** that combines:
- **XGBoost**: Fast, interpretable classifier for signal quality
- **LSTM Neural Network**: Deep learning for temporal pattern recognition
- **Ensemble Predictor**: Weighted combination of both models

This system filters out low-quality MACD crossover signals, improving your win rate and reducing false positives.

---

## üìã Quick Start

### 1. Install Dependencies

```powershell
# Install ML packages
pip install xgboost scikit-learn joblib matplotlib seaborn

# Install PyTorch with CUDA support (for your RTX 4070 Super)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Verify CUDA is available
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
```

### 2. Train Models (One-Time Setup)

**Option A: Quick Test (15-20 minutes)**
```powershell
python train_models.py
# Choose option 1 when prompted
```

**Option B: Full Training (3-4 hours)**
```powershell
python train_models.py
# Choose option 2 for full S&P 500 training
```

**What happens during training:**
1. ‚úÖ Data Collection (10-30 min): Downloads 3 years of historical data
2. ‚úÖ XGBoost Training (2-5 min): Trains on snapshot features
3. ‚úÖ LSTM Training (15-30 min): Trains on price sequences (GPU accelerated)
4. ‚úÖ Creates ensemble predictor

### 3. Enable ML Filtering in Scanner

Edit `macd_scanner.py`:
```python
# Around line 487
USE_ML_FILTER = True              # Enable ML filtering
ML_CONFIDENCE_THRESHOLD = 0.65    # Adjust threshold (0.5-0.95)
ML_TARGET_PERIOD = 5              # Prediction horizon (5, 10, or 20 days)
```

### 4. Run Scanner with ML

```powershell
python macd_scanner.py
```

Signals will now include ML confidence scores and grades (A+, A, B, C, D, F).

---

## üéØ Understanding the ML System

### How It Works

1. **XGBoost Model (40% weight)**
   - Analyzes current indicator values (RSI, ADX, MACD, volume, etc.)
   - Predicts: "Will this signal be profitable in 5 days?"
   - Fast prediction (<1ms per signal)

2. **LSTM Model (60% weight)**
   - Analyzes last 30 days of price/indicator sequences
   - Learns temporal patterns (e.g., "After pattern X, price usually goes up")
   - GPU accelerated prediction (~5ms per signal)

3. **Ensemble Predictor**
   - Combines both predictions with weighted voting
   - Applies confidence threshold to filter signals
   - Grades signals from A+ (best) to F (reject)

### Signal Grading System

| Grade | Confidence | Interpretation | Action |
|-------|-----------|----------------|--------|
| **A+** | 95-100% | Exceptional signal | Strong buy/sell |
| **A**  | 85-95%  | Excellent signal | Buy/sell |
| **B**  | 75-85%  | Good signal | Consider |
| **C**  | 65-75%  | Fair signal | Cautious |
| **D**  | 50-65%  | Poor signal | Likely rejected |
| **F**  | <50%    | Failed signal | Rejected |

### Confidence Threshold Guide

Set `ML_CONFIDENCE_THRESHOLD` based on your trading style:

- **0.75-0.80 (Conservative)**: Fewer signals, higher quality
  - Win rate: 70-80%
  - Signals per scan: 5-15
  
- **0.65-0.70 (Balanced)**: Moderate signals, good quality ‚úÖ **Recommended**
  - Win rate: 60-70%
  - Signals per scan: 10-25
  
- **0.55-0.60 (Aggressive)**: More signals, mixed quality
  - Win rate: 55-65%
  - Signals per scan: 20-40

---

## üìä Training Details

### Data Collection

The system collects historical MACD crossover signals and labels them:

```python
# For each crossover, it extracts:
# 1. Snapshot features (30+ indicators)
macd_value, rsi, adx, volume_ratio, price_momentum, distance_from_ema200, etc.

# 2. Sequence features (30-day history)
[Close, Volume, MACD, Signal, RSI, ADX, BB_Position, EMA_200]

# 3. Outcome labels (forward returns)
profitable_5d, profitable_10d, profitable_20d, return_5d, max_drawdown_5d, etc.
```

**Typical dataset size:**
- 30 stocks (quick test): 500-1500 signals
- S&P 500 (full): 10,000-30,000 signals

### Model Architecture

**XGBoost:**
```
200 trees, max_depth=6, learning_rate=0.05
Features: 35+ engineered indicators
Output: Binary classification (profitable/unprofitable)
```

**LSTM:**
```
Input: (batch, 30 timesteps, 8 features)
LSTM: 128 hidden units, 2 layers
Attention: Multi-head attention mechanism
Output: (batch, 1) probability
```

### Training Time

| Component | Time (30 stocks) | Time (S&P 500) |
|-----------|------------------|----------------|
| Data Collection | 10-15 min | 2-3 hours |
| XGBoost Training | 2 min | 5 min |
| LSTM Training | 15-20 min | 30-45 min |
| **Total** | **20-30 min** | **3-4 hours** |

*LSTM times with RTX 4070 Super. CPU-only would be 3-5x slower.*

---

## üîß Advanced Configuration

### Retrain Models Periodically

Markets evolve, so retrain every 1-3 months:

```powershell
# Re-collect data with latest signals
python data_preparation.py

# Retrain both models
python train_models.py --quick
```

### Train for Different Horizons

You can train models for 5, 10, or 20-day predictions:

```powershell
# Train 10-day model
python ml_model.py  # Edit target_period=10 in script
python lstm_model.py  # Edit target_period=10 in script

# Then update scanner:
ML_TARGET_PERIOD = 10
```

### Optimize Ensemble Weights

Find optimal XGBoost/LSTM weighting for your data:

```python
from ensemble_predictor import EnsemblePredictor
import pandas as pd
import numpy as np

# Load validation data
val_data = pd.read_csv('validation_data.csv')
val_sequences = np.load('validation_sequences.npy')
val_labels = val_data['profitable_5d']

# Initialize ensemble
ensemble = EnsemblePredictor()
ensemble.load_models()

# Optimize weights
best_weights, accuracy = ensemble.optimize_weights(
    validation_data=val_data.to_dict('records'),
    validation_sequences=val_sequences,
    validation_labels=val_labels
)

print(f"Best weights: XGB={best_weights[0]:.1f}, LSTM={best_weights[1]:.1f}")
```

### Hyperparameter Tuning

**XGBoost:**
```python
# Edit ml_model.py, line ~120
model.train(df, optimize_hyperparameters=True)  # Runs grid search
```

**LSTM:**
```python
# Edit lstm_model.py, modify architecture
trainer = LSTMTrainer(
    input_size=8,
    hidden_size=256,      # Increase for more capacity
    num_layers=3,         # Add more layers
    dropout=0.4           # Adjust regularization
)
```

---

## üìà Performance Metrics

After training, review these files:

1. **xgboost_diagnostics_5d.png**
   - ROC curve
   - Feature importance
   - Confusion matrix
   - Probability distribution

2. **lstm_training_history_5d.png**
   - Training/validation loss
   - Accuracy curves
   - AUC over epochs

**Expected Performance:**
- XGBoost Test AUC: 0.65-0.75
- LSTM Test AUC: 0.68-0.78
- Ensemble Test AUC: 0.70-0.80

*Higher is better. >0.70 is good, >0.75 is excellent.*

---

## üêõ Troubleshooting

### Issue: CUDA out of memory

```python
# Reduce LSTM batch size in lstm_model.py, line ~190
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Was 32
```

### Issue: Models not found

```powershell
# Check if model files exist
ls *.pkl  # XGBoost models
ls *.pth  # LSTM models

# If missing, train them:
python train_models.py
```

### Issue: Poor model performance (AUC < 0.60)

**Possible causes:**
1. Not enough training data ‚Üí Collect from more stocks
2. Data quality issues ‚Üí Check for data errors
3. Overfitting ‚Üí Increase dropout, reduce model complexity
4. Market regime changed ‚Üí Retrain with recent data

### Issue: Slow predictions

**LSTM taking too long?**
```python
# Check if using GPU
import torch
print(torch.cuda.is_available())  # Should be True

# If False, reinstall PyTorch with CUDA:
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

---

## üí° Tips & Best Practices

1. **Start with Quick Test**: Train on 30 stocks first to validate setup
2. **Monitor Win Rate**: Track actual signal outcomes to verify model accuracy
3. **Retrain Regularly**: Market conditions change, refresh models quarterly
4. **Adjust Threshold**: Start at 0.65, increase if too many false positives
5. **Review Features**: Check feature importance to understand what matters
6. **GPU is Worth It**: LSTM training is 10-50x faster with GPU
7. **Backtest First**: Test ML-filtered signals on historical data before live trading

---

## üéì Understanding Feature Importance

After training, XGBoost shows which features matter most:

**Typical top features:**
1. `macd_histogram` - Current MACD momentum
2. `adx` - Trend strength
3. `rsi` - Overbought/oversold
4. `distance_from_ema200_pct` - Position vs. trend
5. `volume_ratio_20d` - Volume confirmation
6. `returns_5d` - Recent momentum
7. `histogram_slope` - MACD acceleration

This tells you what the model considers important for profitability.

---

## üìö File Structure

```
MACD Model/
‚îú‚îÄ‚îÄ data_preparation.py          # Data collection script
‚îú‚îÄ‚îÄ ml_model.py                   # XGBoost implementation
‚îú‚îÄ‚îÄ lstm_model.py                 # LSTM neural network
‚îú‚îÄ‚îÄ ensemble_predictor.py         # Combines both models
‚îú‚îÄ‚îÄ train_models.py               # Master training script
‚îú‚îÄ‚îÄ macd_scanner.py               # Scanner with ML integration
‚îú‚îÄ‚îÄ training_data.csv             # Feature snapshots
‚îú‚îÄ‚îÄ training_data_sequences.npy   # LSTM sequences
‚îú‚îÄ‚îÄ xgboost_model_5d.pkl          # Trained XGBoost
‚îú‚îÄ‚îÄ lstm_model_5d.pth             # Trained LSTM
‚îú‚îÄ‚îÄ xgboost_diagnostics_5d.png    # XGBoost performance
‚îî‚îÄ‚îÄ lstm_training_history_5d.png  # LSTM training curves
```

---

## üöÄ Next Steps

1. ‚úÖ **Train models**: Run `python train_models.py`
2. ‚úÖ **Enable ML filtering**: Set `USE_ML_FILTER = True`
3. ‚úÖ **Run scanner**: `python macd_scanner.py`
4. ‚úÖ **Review results**: Check ML grades and confidence scores
5. ‚úÖ **Backtest**: Test on historical data
6. ‚úÖ **Deploy**: Use in production or integrate with Streamlit app

---

## üìû Need Help?

- Check diagnostic plots for model performance
- Review training logs for errors
- Verify CUDA setup for GPU acceleration
- Start with quick test (30 stocks) before full training

Happy trading! üéØüìà
